{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cjman/venv/tfda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4268]])\n",
      "tensor([[-0.5787]])\n",
      "tensor([[1.2804]])\n",
      "tensor([[-0.5787]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)\n",
    "c = torch.tensor([3.], requires_grad=True)\n",
    "w = nn.Linear(1, 1, bias=False)\n",
    "x = nn.Linear(1, 1, bias=False)\n",
    "d = w(a)\n",
    "d = x(d)\n",
    "d.backward()\n",
    "print(w.weight.grad)\n",
    "print(x.weight.grad)\n",
    "x.requires_grad_(False) # requires_grad를 false로 해도 이전 레이어의 gradient를 구하는데 영향을 주진않는다.\n",
    "e = w(b)\n",
    "e = x(e)\n",
    "e.backward()\n",
    "print(w.weight.grad)\n",
    "print(x.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3706]])\n",
      "tensor([[0.5833]])\n",
      "140582953813184\n",
      "140582953813184\n",
      "140582953685632\n",
      "False\n",
      "tensor([[0.3706]])\n",
      "tensor([[0.5833]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)\n",
    "c = torch.tensor([3.], requires_grad=True)\n",
    "w = nn.Linear(1, 1, bias=False)\n",
    "x = nn.Linear(1, 1, bias=False)\n",
    "d = w(a)\n",
    "d = x(d)\n",
    "d.backward()\n",
    "print(w.weight.grad)\n",
    "print(x.weight.grad)\n",
    "e = w(b)\n",
    "print(id(e))\n",
    "with torch.no_grad(): # 이렇게하면 output e가 requires_grad가 false가 되니깐 backward를 실행할 수가없음.\n",
    "    print(id(e))\n",
    "    \n",
    "    e = x(e)\n",
    "    print(id(e))\n",
    "\n",
    "    print(e.requires_grad)\n",
    "    \n",
    "e.backward()\n",
    "print(w.weight.grad)\n",
    "print(x.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3710]])\n",
      "tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "c = torch.tensor([3.], requires_grad=True)\n",
    "w1 = nn.Linear(1, 1, bias=False)\n",
    "w2 = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "a = w1(c)\n",
    "a = w2(a)\n",
    "a.backward()\n",
    "w2.zero_grad()\n",
    "\n",
    "print(w1.weight.grad)\n",
    "print(w2.weight.grad)\n",
    "# 이렇게해도 grad가 none은 아니다.. 그냥 0이나 none이면 문제가 되는건가?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "895f9d8e9dcd01bbf27c3aadf8d2c94adcc46d6e3e500a26f14629471db7d452"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tfda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
